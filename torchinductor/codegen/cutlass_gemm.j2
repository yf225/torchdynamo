import torch
from torchinductor.compile_fx import stream_dict

# System modules
import numpy as np
import os
import os.path
import sys
import ctypes
from types import SimpleNamespace
import time

# CUDA Python modules
from cuda import cuda
from cuda import nvrtc

# CUTLASS modules
import library
import manifest as cutlass_manifest
import generator
import rt

username = os.environ.get('USER')

prebuilt_gemm = None


# TODO(yf225): add autotuning and kernel selection
def compile_cuda_module(
    {% for i in template_inout_argdefs %}
    {{i}},
    {% endfor %}
    M,
    N,
    K,
):
    global prebuilt_gemm

    if prebuilt_gemm is not None:
        return prebuilt_gemm

    cuda_ver = "11.4"
    cuda_arch = "80"  # assuming A100

    # Construct an SGEMM
    manifest = cutlass_manifest.Manifest()
    generator.GenerateSM50_Simt(manifest, cuda_ver)

    # Look up the GEMM operation
    operation = manifest.operations_by_name['cutlass_simt_sgemm_128x128_8x2_nn_align1']

    # Construct a runtime GEMM operation
    prebuilt_gemm = rt.Gemm(operation)

    # Construct a module
    architectures = [80,]
    include_paths = [
      f'/fsx/users/{username}/cutlass/include',
      f'/fsx/users/{username}/cutlass/tools/util/include',
      f'/usr/local/cuda-{cuda_ver}/include',
      f'/usr/local/cuda-{cuda_ver}/targets/x86_64-linux/include',
    ]

    compilation_options = rt.CompilationOptions(architectures, include_paths)
    # NOTE: compilation needs to happen outside of CUDA stream capture (e.g. when capturing for CUDA graph)
    module = rt.Module('module.cu', [prebuilt_gemm], compilation_options)

    return prebuilt_gemm


def {{kernel_name}}(
    {% for i in template_inout_argdefs %}
    {{i}},
    {% endfor %}
    M,
    N,
    K,
    # fusable kernels args
    {% for i in extra_argdefs %}
    {{i}}=None,
    {% endfor %}
):
    global stream_dict

    gemm = compile_cuda_module(
        tensor_A,
        tensor_B,
        M,
        N,
        K,
        tensor_C,
        gemm_bias_out,
    )
    assert gemm is not None

    if "cutlass_stream" not in stream_dict:
        # If currently CUTLASS kernel is not required to use any pre-existing stream,
        # then create a new stream dedicated to CUTLASS which is also exposed publicly
        # for easier control.
        stream_dict["cutlass_stream"] = torch.cuda.Stream()
    cuda_stream_ptr = stream_dict["cutlass_stream"].cuda_stream

    # Formula: D = alpha * (A @ B) + beta * C
    # # TODO(yf225): for some weird reason, no matter what bias tensor we pass into the following GEMM operation,
    # # it will not be used. We need to debug this.
    # tensor_C = torch.arange(M*N, device='cuda', dtype=tensor_A.dtype).view(M, N)

    arguments = rt.GemmArguments()
    arguments.problem_size = rt.GemmCoord(M, N, K)
    arguments.output_op.alpha = 1.0  # TODO(yf225): pass in from outside
    if bias is not None:
        arguments.output_op.beta = 1.0  # TODO(yf225): pass in from outside
    else:
        arguments.output_op.beta = 0.0
    arguments.A = rt.TensorRef(tensor_A.data_ptr(), tensor_A.stride()[0])
    arguments.B = rt.TensorRef(tensor_B.data_ptr(), tensor_B.stride()[0])
    arguments.C = rt.TensorRef(tensor_C.data_ptr(), tensor_C.stride()[0])
    arguments.D = rt.TensorRef(gemm_bias_out.data_ptr(), gemm_bias_out.stride()[0])

    host_workspace = bytearray(gemm.get_host_workspace_size(arguments))
    device_workspace = None

    # TODO(yf225): maybe we can save overhead by doing plan and initialize only once?
    # but this overhead only affects non-cudagraph case though.
    launch_config = gemm.plan(arguments)

    # `gemm.initialize()` doesn't actually need the `stream` argument, so passing None
    byte_count = gemm.initialize(host_workspace, device_workspace, launch_config, arguments, stream=None)

    err = gemm.run(host_workspace, device_workspace, launch_config, stream=cuda_stream_ptr)
    if err != cuda.CUresult.CUDA_SUCCESS:
      raise RuntimeError('CUDA Error %s' % str(err))
