import torch
import jinja2

generated_kernel = jinja2.Template("""
# ... The generated GEMM epilogue fusion kernel ...
# ... 
{% if pointwise_code %}
{{ pointwise_code | indent(4, true) }}
{% endif %}
# ...
""")

# TODO(yf225): Can we use TorchDynamo / TorchInductor caching mechanism to reuse kernel for subsequent runs?
@hpcfuser.cached
def {{kernel_name}}(
    {% for i in template_inout_argdefs %}
    {{i}},
    {% endfor %}
):
    # TODO(yf225): this requires just-in-time complication, see if can reuse CUTLASS-Python infra.
    return hpcfuser.autotune(configs=[...])(generated_kernel)(
        {% for i in template_inout_argdefs %}
        {{i}},
        {% endfor %}
    )
