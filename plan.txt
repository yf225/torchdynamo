# Keep this branch in sync with https://github.com/jansel/torchdynamo/ "inductor" branch.

# For benchmark, use https://github.com/yf225/benchmark repo (which is essentially the same as https://github.com/jansel/benchmark).

-------

export PATH="/opt/slurm/bin:$PATH"

srun --nodes=1 --gpus-per-node=8 -p dev -t 12:00:00 --exclusive --pty /bin/bash -l
srun --nodes=4 --gpus-per-node=8 -p train -t 12:00:00 --exclusive --pty /bin/bash -l
srun --nodes=8 --gpus-per-node=8 -p scavenge -t 72:00:00 --exclusive --pty /bin/bash -l
srun --nodes=8 --gpus-per-node=8 -p mae -t 72:00:00 --exclusive --pty /bin/bash -l

export HOME_DIR=/fsx-mae/$USER  # FAIR AWS cluster
# export HOME_DIR=/fsx/users/$USER  # AI AWS cluster

conda deactivate && conda deactivate && conda deactivate
. /fsx/users/willfeng/conda/etc/profile.d/conda.sh
export PATH="/fsx/users/willfeng/conda/bin:$PATH"
conda deactivate && conda deactivate && conda deactivate
conda activate torch_nightly_cuda
export MODULEPATH=/data/shared/modulefiles:$MODULEPATH
module unload cuda
module unload nccl
module unload nccl_efa
module load cuda/11.4 nccl/2.12.7-cuda.11.4 nccl_efa/1.2.0-nccl.2.12.7-cuda.11.4
source /data/shared/bin/cluster_env_new.sh
source /data/home/willfeng/setup_efa.sh
export CUDA_HOME=/usr/local/cuda-11.4
export NCCL_INCLUDE_DIR=/usr/local/cuda-11.4/include

# First time
# pip3 install --pre torch torchtext torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/cu113
# cd ~/torchdynamo
# pip install -r requirements.txt
# python setup.py develop

# In fairinternal torchbench repo
# python install.py

cd torchdynamo_yf225

# fix the version of black so `make format` / `make lint` work
make lint-deps

# make sure it works
cd benchmarks/
./torchbench.py --inductor --fast --devices cuda

# TODO: then, run all ranking models with Triton vs. NVFuser (see VERY_SLOW_BENCHMARKS in dynamo repo)
